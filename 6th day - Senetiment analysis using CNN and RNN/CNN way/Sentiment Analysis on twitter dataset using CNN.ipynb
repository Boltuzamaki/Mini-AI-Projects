{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from contraction import CONTRACTION_MAP     # Its a py file contain expanded word of all short words like I'm\n",
    "from bs4 import BeautifulSoup          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "# Download Sentiment 140 dataset from https://www.kaggle.com/kazanova/sentiment140\n",
    "data = pd.read_csv(\"data_real.csv\")\n",
    "data.head()\n",
    "data = data.replace(4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning funstions\n",
    "\n",
    "def remove_htmltags(text):                    # Remove HTML tags\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):             # Normalizing accented charaters like Ã¼\n",
    "    import unicodedata\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP): # Expanding short words iike I've --> I have\n",
    "    from contraction import CONTRACTION_MAP\n",
    "    import contraction\n",
    "    import re\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):              # Remove special characters\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def simple_stemmer(text):                                             # Stemming the words\n",
    "    import nltk\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def simple_lemmatize(text):                                          # lammetizing the words\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):                     # Remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import WordPunctTokenizer\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    stopword_list =stopwords.words('english')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def remove_hash_attherate(text):                                         # Remove @ and # tags\n",
    "    text = re.sub(\"#\\w*\", \"\",text)\n",
    "    text = re.sub(\"@\\w*\", \"\",text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# Compiling all text cleaning function\n",
    "\n",
    "def noramalize_text(document,htmltags = True, accented_chars = True, contractions_exp = True,\n",
    "                   text_lower_case = True,special_characters = True, stemmer_text = True, \n",
    "                   lemmatize_text = True, stopwords_remove = False, remove_hash = True):\n",
    "    \n",
    "    normalized_doc = []\n",
    "    \n",
    "    for text in document:\n",
    "        if htmltags:\n",
    "            text = remove_htmltags(text)\n",
    "        \n",
    "        if accented_chars:\n",
    "            text = remove_accented_chars(text)\n",
    "        \n",
    "        if contractions_exp:\n",
    "            text = expand_contractions(text)\n",
    "        \n",
    "        if text_lower_case:\n",
    "            text = text.lower()\n",
    "        # remove extra line\n",
    "        text = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',text)\n",
    "        \n",
    "        if remove_hash:\n",
    "            text = remove_hash_attherate(text)\n",
    "            \n",
    "        if special_characters:\n",
    "            text = remove_special_characters(text)\n",
    "            \n",
    "        if stemmer_text:\n",
    "            text = simple_stemmer(text)\n",
    "        \n",
    "        if lemmatize_text:\n",
    "            text = simple_lemmatize(text)\n",
    "        \n",
    "        # remove extra whitespace\n",
    "        text = re.sub(' +', ' ', text)   \n",
    "        \n",
    "        if stopwords_remove:\n",
    "            text = remove_stopwords(text)\n",
    "            \n",
    "        normalized_doc.append(text)    \n",
    "        \n",
    "    return normalized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column with cleaned tweets \n",
    "\n",
    "data[\"cleaned\"] = noramalize_text(data['tweets'],stemmer_text = False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving it to csv for fast load\n",
    "\n",
    "data.to_csv(\"cleaned.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loding cleaned csv\n",
    "\n",
    "data = pd.read_csv(\"cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Typecasting as string\n",
    "\n",
    "text = data[\"cleaned\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the text using keras preprocessing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN =50\n",
    "\n",
    "def Tokenizing(data, tokenize_label = \"\", label = \"\"):\n",
    "    # Spilitting in test and test data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    \n",
    "    # Tokenizing using keras preprocessing library\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences_train = tokenizer.texts_to_sequences(train[\"tweets\"])\n",
    "    \n",
    "    sequences_test = tokenizer.texts_to_sequences(test[\"tweets\"])\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(vocab_size)\n",
    "    Y_train = train[\"labels\"]\n",
    "    Y_test = test[\"labels\"]\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(sequences_train,\n",
    "                                                           value = 0,\n",
    "                                                           padding = 'post',\n",
    "                                                           maxlen = MAX_LEN)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(sequences_test,\n",
    "                                                           value = 0,\n",
    "                                                           padding = 'post',\n",
    "                                                           maxlen = MAX_LEN)\n",
    "    \n",
    "    return X_test, X_train, Y_train, Y_test, tokenizer, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469332\n"
     ]
    }
   ],
   "source": [
    "X_test, X_train, Y_train, Y_test,tokenizer, vocab_size = Tokenizing(data, \"tweets\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the tokenizer into pickle for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the CNN 1D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 100\n",
    "nb_filters = 256\n",
    "FFN_units = 512\n",
    "nb_classes = 2\n",
    "dropout_rate = 0.1\n",
    "Batch_size = 64\n",
    "kernel_size_first  = 2 # First one\n",
    "# Creating the CNN \n",
    "\n",
    "def Model_Conv():\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Conv1D, GlobalMaxPool1D, Dropout, Embedding, Dense,Input,MaxPooling1D, GlobalMaxPooling1D,Concatenate\n",
    "    from keras.models import Model\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    text_input_layer = Input(shape=(MAX_LEN,))\n",
    "    embedding = Embedding(vocab_size, embedding_dims)(text_input_layer)\n",
    "    conv1 = Conv1D(filters = nb_filters,kernel_size = kernel_size_first,padding = \"valid\",\n",
    "                    activation = \"relu\")(embedding) \n",
    "    pool1 = GlobalMaxPooling1D()(conv1)\n",
    "    conv2 = Conv1D(filters = nb_filters,kernel_size = kernel_size_first+1,padding = \"valid\",\n",
    "                    activation = \"relu\")(embedding)\n",
    "    pool2  = GlobalMaxPooling1D()(conv2)\n",
    "    conv3 = Conv1D(nb_filters,kernel_size = kernel_size_first+2,padding = \"valid\",\n",
    "                activation = \"relu\")((embedding))\n",
    "    pool3 = GlobalMaxPooling1D()(conv3)\n",
    "    cat_conv = Concatenate(axis=1)([pool1,pool2,pool3])\n",
    "    dense1 = Dense(units = FFN_units, activation = \"relu\" )(cat_conv)\n",
    "    drop = Dropout(rate = dropout_rate )(dense1)\n",
    "    last = Dense(units = 1, activation = \"sigmoid\")(drop)\n",
    "    mod  = Model(text_input_layer, last)\n",
    "    \n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod  = Model_Conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 100)      46933200    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 49, 256)      51456       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 48, 256)      77056       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 47, 256)      102656      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          393728      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            513         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 47,558,609\n",
      "Trainable params: 47,558,609\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1279999/1279999 [==============================] - 1797s 1ms/step - loss: 0.4248 - acc: 0.8026\n",
      "Epoch 2/2\n",
      "1279999/1279999 [==============================] - 1743s 1ms/step - loss: 0.3579 - acc: 0.8416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a12604400>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.fit(X_train, Y_train, batch_size =Batch_size, epochs = 2 , verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000/320000 [==============================] - 15s 46us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4028938247174025, 0.81696875]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.evaluate(X_test ,Y_test, batch_size =Batch_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the tokenizer pickle and test it on any sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"This movie is not so good\", \"This suppose to be bad but it is good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(text)\n",
    "test = tf.keras.preprocessing.sequence.pad_sequences(sequences_test,value = 0,padding = 'post', maxlen = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mod.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_list = []\n",
    "\n",
    "for i in range(0,pred.shape[0]):\n",
    "    if pred[i]>0.5:\n",
    "        value = \"Positive\"\n",
    "    if pred[i]<=0.5:\n",
    "        value = \"Negative\"\n",
    "    sentiment_list.append(value)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Negative', 'Positive']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
