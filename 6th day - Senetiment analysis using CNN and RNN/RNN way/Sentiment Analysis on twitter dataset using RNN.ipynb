{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,SimpleRNN, GRU, Dropout,Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from contraction import CONTRACTION_MAP     # Its a py file contain expanded word of all short words like I'm\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_real.csv\")\n",
    "data.head()\n",
    "data = data.replace(4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning functions\n",
    "\n",
    "def remove_htmltags(text):                      # Remove HTML tags\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):               # Normalizing accented charaters like Ã¼\n",
    "    import unicodedata\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP): # Expanding short words iike I've --> I have\n",
    "    from contraction import CONTRACTION_MAP\n",
    "    import contraction\n",
    "    import re\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):              # Remove special characters\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def simple_stemmer(text):                                            # Stemming the words\n",
    "    import nltk\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def simple_lemmatize(text):                                          # lammetizing the words\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):                    # Remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import WordPunctTokenizer\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    stopword_list =stopwords.words('english')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def remove_hash_attherate(text):                                # Removing hastags and @ tags\n",
    "    text = re.sub(\"#\\w*\", \"\",text)\n",
    "    text = re.sub(\"@\\w*\", \"\",text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def noramalize_text(document,htmltags = True, accented_chars = True, contractions_exp = True,\n",
    "                   text_lower_case = True,special_characters = True, stemmer_text = True, \n",
    "                   lemmatize_text = True, stopwords_remove = False, remove_hash = True):\n",
    "    \n",
    "    normalized_doc = []\n",
    "    \n",
    "    for text in document:\n",
    "        if htmltags:\n",
    "            text = remove_htmltags(text)\n",
    "        \n",
    "        if accented_chars:\n",
    "            text = remove_accented_chars(text)\n",
    "        \n",
    "        if contractions_exp:\n",
    "            text = expand_contractions(text)\n",
    "        \n",
    "        if text_lower_case:\n",
    "            text = text.lower()\n",
    "        # remove extra line\n",
    "        text = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',text)\n",
    "        \n",
    "        if remove_hash:\n",
    "            text = remove_hash_attherate(text)\n",
    "            \n",
    "        if special_characters:\n",
    "            text = remove_special_characters(text)\n",
    "            \n",
    "        if stemmer_text:\n",
    "            text = simple_stemmer(text)\n",
    "        \n",
    "        if lemmatize_text:\n",
    "            text = simple_lemmatize(text)\n",
    "        \n",
    "        # remove extra whitespace\n",
    "        text = re.sub(' +', ' ', text)   \n",
    "        \n",
    "        if stopwords_remove:\n",
    "            text = remove_stopwords(text)\n",
    "            \n",
    "        normalized_doc.append(text)    \n",
    "        \n",
    "    return normalized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\boltuzamaki\\anaconda3\\envs\\pose\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \" i just received my G8 viola exam.. and its... well... .. disappointing.. :\\..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n",
      "c:\\users\\boltuzamaki\\anaconda3\\envs\\pose\\lib\\site-packages\\bs4\\__init__.py:312: UserWarning: \"E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\../  \\../\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % self._decode_markup(markup)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he cannot update his facebook by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many time for the ball managed to save...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feel itchy and like it on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "      <td>no it is not behaving at all i am mad why am i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>0</td>\n",
       "      <td>not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  labels  \\\n",
       "0  is upset that he can't update his Facebook by ...       0   \n",
       "1  @Kenichan I dived many times for the ball. Man...       0   \n",
       "2    my whole body feels itchy and like its on fire        0   \n",
       "3  @nationwideclass no, it's not behaving at all....       0   \n",
       "4                      @Kwesidei not the whole crew        0   \n",
       "\n",
       "                                             cleaned  \n",
       "0  is upset that he cannot update his facebook by...  \n",
       "1  i dived many time for the ball managed to save...  \n",
       "2       my whole body feel itchy and like it on fire  \n",
       "3  no it is not behaving at all i am mad why am i...  \n",
       "4                                 not the whole crew  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"cleaned\"] = noramalize_text(data['tweets'],stemmer_text = False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"cleaned.csv\", index = None)\n",
    "data = pd.read_csv(\"cleaned.csv\")\n",
    "text = data[\"cleaned\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN =50\n",
    "\n",
    "def Tokenizing(data, tokenize_label = \"\", label = \"\"):\n",
    "    # Spilitting in test and test data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    \n",
    "    # Tokenizing using keras preprocessing library\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences_train = tokenizer.texts_to_sequences(train[\"tweets\"])\n",
    "    \n",
    "    sequences_test = tokenizer.texts_to_sequences(test[\"tweets\"])\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(vocab_size)\n",
    "    Y_train = train[\"labels\"]\n",
    "    Y_test = test[\"labels\"]\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(sequences_train,\n",
    "                                                           value = 0,\n",
    "                                                           padding = 'post',\n",
    "                                                           maxlen = MAX_LEN)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(sequences_test,\n",
    "                                                           value = 0,\n",
    "                                                           padding = 'post',\n",
    "                                                           maxlen = MAX_LEN)\n",
    "    \n",
    "    return X_test, X_train, Y_train, Y_test, tokenizer, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469399\n"
     ]
    }
   ],
   "source": [
    "X_test, X_train, Y_train, Y_test, tokenizer, vocab_size= Tokenizing(data, \"tweets\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the tokenizer for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word = vocab_size\n",
    "input_length = 50\n",
    "dropout_val = 0.2\n",
    "Dense_layers = 256\n",
    "RNN_layer = 128\n",
    "no_of_embeddings = 100\n",
    "max_pad_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_RNN(layer = \"RNN\"):\n",
    "    if layer == \"RNN\":\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(num_word +1 ,  no_of_embeddings ,input_length=max_pad_length))\n",
    "       # model.add(SimpleRNN(RNN_layer,return_sequences=True))\n",
    "       # model.add(Dropout(dropout_val))\n",
    "        model.add(SimpleRNN(RNN_layer))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(Dense_layers, activation = 'relu'))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(1 , activation = 'sigmoid'))\n",
    "    \n",
    "    if layer == \"LSTM\":\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(num_word +1 ,  no_of_embeddings ,input_length=max_pad_length))\n",
    "        model.add(LSTM(RNN_layer,return_sequences=True))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(LSTM(RNN_layer))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(Dense_layers, activation = 'relu'))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(1 , activation = 'sigmoid'))\n",
    "        \n",
    "    if layer == \"GRU\":\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(num_word +1 ,  no_of_embeddings ,input_length=max_pad_length))\n",
    "        model.add(GRU(RNN_layer,return_sequences=True))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(GRU(RNN_layer))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(Dense_layers, activation = 'relu'))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(1 , activation = 'sigmoid'))\n",
    "        \n",
    "    if layer == \"BILSTM\":\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(num_word +1 ,  no_of_embeddings ,input_length=max_pad_length))\n",
    "        model.add(Bidirectional(LSTM(RNN_layer,return_sequences=True)))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Bidirectional(LSTM(RNN_layer)))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(Dense_layers, activation = 'relu'))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(1 , activation = 'sigmoid'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model_RNN(layer = \"RNN\")        # Choose layers anyone from LSTM, GRU , BILSTM , and simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.fit(X_train, Y_train, batch_size =Batch_size, epochs = 2 , verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.evaluate(X_test ,Y_test, batch_size =Batch_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the tokenizer pickle and test it on any sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"This movie is not so good\", \"This suppose to be bad but it is good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(text)\n",
    "test = tf.keras.preprocessing.sequence.pad_sequences(sequences_test,value = 0,padding = 'post', maxlen = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mod.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_list = []\n",
    "\n",
    "for i in range(0,pred.shape[0]):\n",
    "    if pred[i]>0.5:\n",
    "        value = \"Positive\"\n",
    "    if pred[i]<=0.5:\n",
    "        value = \"Negative\"\n",
    "    sentiment_list.append(value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
