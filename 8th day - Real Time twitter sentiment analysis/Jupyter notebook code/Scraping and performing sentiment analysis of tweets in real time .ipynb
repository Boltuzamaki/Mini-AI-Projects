{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contraction import CONTRACTION_MAP\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from contraction import CONTRACTION_MAP     # Its a py file contain expanded word of all short words like I'm\n",
    "from bs4 import BeautifulSoup\n",
    "from tweepy import Stream\n",
    "from tweepy import StreamListener\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import tweepy\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmltags(text):                    # Remove HTML tags\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):             # Normalizing accented charaters like Ã¼\n",
    "    import unicodedata\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP): # Expanding short words iike I've --> I have\n",
    "    from contraction import CONTRACTION_MAP\n",
    "    import contraction\n",
    "    import re\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):              # Remove special characters\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def simple_stemmer(text):                                             # Stemming the words\n",
    "    import nltk\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def simple_lemmatize(text):                                          # lammetizing the words\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):                     # Remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import WordPunctTokenizer\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    stopword_list =stopwords.words('english')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def remove_link(text):                                                   # Remove https\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "    \n",
    "def remove_hash_attherate(text):                                         # Remove @ and # tags\n",
    "    text = re.sub(\"#\\w*\", \"\",text)\n",
    "    text = re.sub(\"@\\w*\", \"\",text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# Compiling all text cleaning function\n",
    "\n",
    "def noramalize_text(text,htmltags = True, accented_chars = True, contractions_exp = True,\n",
    "                   text_lower_case = True,special_characters = True, stemmer_text = True, \n",
    "                   lemmatize_text = True, stopwords_remove = False, remove_hash = True, remove_linkadd = True):\n",
    "    if htmltags:\n",
    "        text = remove_htmltags(text)\n",
    "        \n",
    "    if accented_chars:\n",
    "        text = remove_accented_chars(text)\n",
    "        \n",
    "    if contractions_exp:\n",
    "        text = expand_contractions(text)\n",
    "        \n",
    "    if text_lower_case:\n",
    "            text = text.lower()\n",
    "    \n",
    "    if remove_linkadd:\n",
    "        text = remove_link(text)\n",
    "    # remove extra line\n",
    "    text = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',text)\n",
    "        \n",
    "    if remove_hash:\n",
    "        text = remove_hash_attherate(text)\n",
    "            \n",
    "    if special_characters:\n",
    "        text = remove_special_characters(text)\n",
    "            \n",
    "    if stemmer_text:\n",
    "        text = simple_stemmer(text)\n",
    "        \n",
    "    if lemmatize_text:\n",
    "        text = simple_lemmatize(text)\n",
    "        \n",
    "    # remove extra whitespace\n",
    "    text = re.sub(' +', ' ', text)   \n",
    "        \n",
    "    if stopwords_remove:\n",
    "        text = remove_stopwords(text) \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the pretrained tokenizer and model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "MAX_LEN = 50    \n",
    "\n",
    "from keras.models import load_model\n",
    "mod = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(['This is good'])\n",
    "test = tf.keras.preprocessing.sequence.pad_sequences(sequences_test,value = 0,padding = 'post', maxlen = MAX_LEN)\n",
    "pred = mod.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get it from twitter developer dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a function to write csv file of results to use it in plotting graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_creator(sentiment_list):\n",
    "    dictionary = { \"sentiment\" : sentiment_list\n",
    "        }\n",
    "    data = pd.DataFrame(dictionary, index = None)\n",
    "    data.to_csv(\"real_time.csv\", index = None)\n",
    "\n",
    "import time    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the tweets and predicting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "class Listener(StreamListener):\n",
    "    def __init__(self):\n",
    "        self.sentiment = 0\n",
    "        self.list = []\n",
    "    def on_data(self, data):\n",
    "        raw_tweets = json.loads(data)\n",
    "        try:\n",
    "            if  not raw_tweets['text'].startswith('RT'):              # \"RT\" to remove retweets\n",
    "                text.append(noramalize_text(raw_tweets['text']))\n",
    "                sequences_test = tokenizer.texts_to_sequences(text)\n",
    "                test = tf.keras.preprocessing.sequence.pad_sequences(sequences_test,value = 0,padding = 'post', maxlen = MAX_LEN)\n",
    "                pred = mod.predict(test)\n",
    "                if pred < 0.5:\n",
    "                    self.sentiment = self.sentiment - 1\n",
    "                if pred >= 0.5:\n",
    "                    self.sentiment = self.sentiment + 1\n",
    "                self.list.append(self.sentiment)  \n",
    "                csv_creator(self.list)                      # Passing predicted list to csv_creator function\n",
    "                time.sleep(2)\n",
    "                print(self.sentiment)\n",
    "                print(noramalize_text(raw_tweets['text']))\n",
    "                text.pop()\n",
    "                \n",
    "        except:\n",
    "            print(\"Error got\")\n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put your authentication details here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key ,consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start real time tweet collecting steam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "thank trump\n",
      "0\n",
      "sinc when do we let communist china dictat how we prevent prepar our own nation against a pandem\n",
      "-1\n",
      "start to get realli uneasi that is in full court press on do not think for one minut they d\n",
      "0\n",
      "well with the fals info from china and the carefre reaction from the left\n",
      "1\n",
      "\n",
      "2\n",
      "2 thi covid19 crisi is anoth exampl from trump a to whi each candid for presiden\n",
      "3\n",
      "i am still wait for all those subpoena you were suppos to issu\n",
      "4\n",
      "you have lost your damn mind move to chinapleas\n",
      "5\n",
      "uh i order nitrat glove through wallmart which were nitrat to protect a vulnerab\n",
      "4\n",
      "the unit state block a un secur council resolut call for a global ceasefir after china push for t\n",
      "5\n",
      "it better than be told what to do like in china\n",
      "6\n",
      "trump cut pandem teamsent ppe to china janmargolf 6 times9 ralli trump\n",
      "7\n",
      "nevertheless scream white priveleg play right into the hand of the right scream corr\n",
      "6\n",
      "state of thi\n",
      "5\n",
      "technic review thi screenshot when get some minut thi outbreak start in china last de\n",
      "6\n",
      "chingon\n",
      "5\n",
      "im person sick and fed up with these idiot chang law and use the china viru to do so\n",
      "6\n",
      "african in china we face coronaviru discrimin\n",
      "7\n",
      "african in china we face coronaviru discrimin\n",
      "6\n",
      "what a clickbait titl tri to ride on the pandem that wa an old news and onli a hand of isol c\n",
      "7\n",
      "prais china who shameless prais america who is the best\n",
      "6\n",
      "when u produs substandard product u bound to a con act china alway cheat th\n",
      "5\n",
      "u coupl nightmar held in china away from daughter\n",
      "4\n",
      "on the first shift suppli chain or diversifi them by ad altern is clearli real but aif they sh\n",
      "3\n",
      "veri reminisc of the missil pariti question between the usa the ussr back in the 70 80 who had the m\n",
      "4\n",
      "a littl over 2 minut of\n",
      "3\n",
      "i am start to realiz what justic is go to mean 60million on either side demsdeepstatechina etc dem d\n",
      "4\n",
      "trump cut pandem teamsent ppe to china janmargolf 6 times9 ralli trump\n",
      "5\n",
      "3rd china 4th usa 5th brazil you are good though\n",
      "6\n",
      "just ad\n",
      "7\n",
      "how are we continu our project remot lot of zoom meet share document and constant commun on\n",
      "8\n",
      "that veri much psbl did d best to contain other countri didnt heed to advisori\n",
      "7\n",
      "if it wa not for china we would not be in thi situat china can go fuck itself\n",
      "6\n",
      "mask factori close down in china ccp viru coronaviru covid19 via fo\n",
      "5\n",
      "the differ between china and australia is that the manipul of china medium is no secret chine ppl und\n",
      "4\n",
      "immedi on restart china cut electr tariff to industri by half forget\n",
      "5\n",
      "could not agre more we also focus on china a did the u and did not think of the european count\n",
      "6\n",
      "nope thing turn into a big boat anchor all those disk just suck the hp out of a tractor and if yo\n",
      "7\n",
      "is thi real life thank you so much everyon\n"
     ]
    }
   ],
   "source": [
    "twitter_stream = Stream(auth, Listener())\n",
    "twitter_stream.filter(languages = [\"en\"], track = ['China'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
